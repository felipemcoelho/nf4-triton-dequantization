{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NF4 Triton Dequantization Benchmarks\n",
    "\n",
    "Technical evaluation of NF4 dequantization performance using Triton kernels vs existing implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "\n",
    "# Clone and install the repo if running in Colab\n",
    "import os\n",
    "if not os.path.exists('nf4_triton_dequantization'):\n",
    "    !git clone https://github.com/felipemcoelho/nf4-triton-dequantization.git\n",
    "    %cd nf4-triton-dequantization\n",
    "    !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    major_version, minor_version = torch.cuda.get_device_capability()\n",
    "    print(f\"CUDA capability: {major_version}.{minor_version}\")\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    HAS_BFLOAT16 = (major_version >= 8)\n",
    "    print(f\"bfloat16 support: {HAS_BFLOAT16}\")\n",
    "else:\n",
    "    print(\"CUDA not available. This benchmark requires a CUDA-capable GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NF4 Quantization Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import Linear4bit\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "def bnb_Linear4bit(hd, m, dtype=torch.float16):\n",
    "    return Linear4bit(\n",
    "        hd, m, bias=None,\n",
    "        compute_dtype=dtype,\n",
    "        compress_statistics=True,\n",
    "        quant_type=\"nf4\",\n",
    "    )\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n",
    "        super().__init__()\n",
    "        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n",
    "        self.up_proj = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n",
    "        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).to(\"cuda\")\n",
    "        self.gate_proj.weight.quant_state.dtype = dtype\n",
    "        self.up_proj.weight.quant_state.dtype = dtype\n",
    "        self.down_proj.weight.quant_state.dtype = dtype\n",
    "        self.act_fn = ACT2FN[\"silu\"]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "# Create a sample MLP with NF4 weights\n",
    "set_seed(42)\n",
    "hd, m = 1024, 4096\n",
    "dtype = torch.float16\n",
    "mlp = MLP(hd=hd, m=m, dtype=dtype)\n",
    "\n",
    "# Create a sample input\n",
    "batch_size, seq_len = 2, 128\n",
    "x = torch.randn((batch_size, seq_len, hd), device=\"cuda\", dtype=dtype)\n",
    "\n",
    "# Examine the weight structure\n",
    "print(f\"Weight dtype: {mlp.gate_proj.weight.dtype}\")\n",
    "print(f\"Weight shape: {mlp.gate_proj.weight.shape}\")\n",
    "print(f\"Compute dtype: {mlp.gate_proj.weight.quant_state.dtype}\")\n",
    "print(f\"Blocksize: {mlp.gate_proj.weight.quant_state.blocksize}\")\n",
    "print(f\"Secondary blocksize: {mlp.gate_proj.weight.quant_state.state2.blocksize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Dequantization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.kernels.utils import fast_dequantize\n",
    "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
    "\n",
    "def unsloth_dequantize(weight):\n",
    "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
    "\n",
    "# Test Unsloth dequantization\n",
    "start = time.time()\n",
    "dequant_weight = unsloth_dequantize(mlp.gate_proj)\n",
    "torch.cuda.synchronize()\n",
    "unsloth_time = time.time() - start\n",
    "\n",
    "print(f\"Unsloth dequantization time: {unsloth_time*1000:.2f} ms\")\n",
    "print(f\"Dequantized dtype: {dequant_weight.dtype}\")\n",
    "print(f\"Dequantized shape: {dequant_weight.shape}\")\n",
    "\n",
    "# Test PEFT dequantization\n",
    "start = time.time()\n",
    "dequant_weight_peft = peft_dequantize(mlp.gate_proj)\n",
    "torch.cuda.synchronize()\n",
    "peft_time = time.time() - start\n",
    "\n",
    "print(f\"PEFT dequantization time: {peft_time*1000:.2f} ms\")\n",
    "print(f\"Results match: {torch.allclose(dequant_weight, dequant_weight_peft)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Triton Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nf4_triton_dequantization import triton_dequantize_nf4\n",
    "\n",
    "# Test Triton implementation\n",
    "start = time.time()\n",
    "dequant_weight_triton = triton_dequantize_nf4(mlp.gate_proj)\n",
    "torch.cuda.synchronize()\n",
    "triton_time = time.time() - start\n",
    "\n",
    "print(f\"Triton dequantization time: {triton_time*1000:.2f} ms\")\n",
    "print(f\"Results match Unsloth: {torch.allclose(dequant_weight, dequant_weight_triton)}\")\n",
    "print(f\"Results match PEFT: {torch.allclose(dequant_weight_peft, dequant_weight_triton)}\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup_vs_unsloth = unsloth_time / triton_time\n",
    "speedup_vs_peft = peft_time / triton_time\n",
    "\n",
    "print(f\"\\nSpeedup vs Unsloth: {speedup_vs_unsloth:.2f}x\")\n",
    "print(f\"Speedup vs PEFT: {speedup_vs_peft:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import run_benchmarks, plot_benchmarks\n",
    "\n",
    "# Run benchmarks (reduced iterations for quicker notebook execution)\n",
    "results = run_benchmarks(iterations=100, warmup=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plot_benchmarks(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(X, mlp, fx):\n",
    "    up = X @ fx(mlp.up_proj).t()\n",
    "    gate = X @ fx(mlp.gate_proj).t()\n",
    "    h = mlp.act_fn(gate) * up\n",
    "    down = h @ fx(mlp.down_proj).t()\n",
    "    return down\n",
    "\n",
    "# Create a larger input for more realistic testing\n",
    "batch_size, seq_len = 4, 512\n",
    "x_large = torch.randn((batch_size, seq_len, hd), device=\"cuda\", dtype=dtype)\n",
    "\n",
    "# Measure time for Unsloth-based forward pass\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "output_unsloth = mlp_forward(x_large, mlp, unsloth_dequantize)\n",
    "torch.cuda.synchronize()\n",
    "unsloth_forward_time = time.time() - start\n",
    "\n",
    "# Measure time for Triton-based forward pass\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "output_triton = mlp_forward(x_large, mlp, triton_dequantize_nf4)\n",
    "torch.cuda.synchronize()\n",
    "triton_forward_time = time.time() - start\n",
    "\n",
    "# Calculate end-to-end speedup\n",
    "forward_speedup = unsloth_forward_time / triton_forward_time\n",
    "\n",
    "print(f\"Results match in forward pass: {torch.allclose(output_unsloth, output_triton)}\")\n",
    "print(f\"Unsloth forward time: {unsloth_forward_time*1000:.2f} ms\")\n",
    "print(f\"Triton forward time: {triton_forward_time*1000:.2f} ms\")\n",
    "print(f\"End-to-end speedup: {forward_speedup:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
