--- a/nf4_triton_dequantization/kernel.py
+++ b/nf4_triton_dequantization/kernel.py
@@ -89,11 +89,15 @@
     low_scaled = low_vals * scale
     high_scaled = high_vals * scale
     
-    # Store interleaved results using unrolled loop
-    for i in tl.static_range(32):
-        idx = i * 2
-        if col_start + idx < n:
-            tl.store(output_ptr + output_base + idx, low_scaled[i])
-        if col_start + idx + 1 < n:
-            tl.store(output_ptr + output_base + idx + 1, high_scaled[i])
+    # Vectorized interleaved store (fixed - no tl.static_range)
+    even_offs = offsets * 2
+    odd_offs = even_offs + 1
+    
+    even_mask = (col_start + even_offs) < n
+    odd_mask = (col_start + odd_offs) < n
+    
+    tl.store(output_ptr + output_base + even_offs, low_scaled, mask=even_mask)
+    tl.store(output_ptr + output_base + odd_offs, high_scaled, mask=odd_mask)
 
 
+# Add PyTorch fallback function
+def pure_torch_fallback(module):
+    """Pure PyTorch fallback for environments where Triton is slow"""
+    weight = module.weight
+    quant_state = weight.quant_state
+    
+    qweight = weight.data
+    absmax = quant_state.absmax
+    absmax32 = quant_state.state2.absmax
+    dtype = quant_state.dtype
+    device = qweight.device
+    
+    m = module.out_features
+    n = module.in_features
+    
+    # NF4 lookup table
+    lut = torch.tensor([
+        -1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,
+        -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0,
+        0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224,
+        0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0
+    ], dtype=torch.float32, device=device)
+    
+    blocks_per_row = (n + 63) // 64
+    absmax32_per_row = (blocks_per_row + 3) // 4
+    
+    # Reshape scales
+    if absmax.dim() == 1:
+        if absmax.numel() == blocks_per_row:
+            absmax = absmax.unsqueeze(0).expand(m, -1)
+        elif absmax.numel() == m * blocks_per_row:
+            absmax = absmax.view(m, blocks_per_row)
+    
+    if absmax32.dim() == 1:
+        if absmax32.numel() == absmax32_per_row:
+            absmax32 = absmax32.unsqueeze(0).expand(m, -1)
+        elif absmax32.numel() == m * absmax32_per_row:
+            absmax32 = absmax32.view(m, absmax32_per_row)
+    
+    # Ensure contiguous
+    qweight = qweight.contiguous()
+    absmax = absmax.contiguous().float()
+    absmax32 = absmax32.contiguous().float()
+    
+    # Process row by row for accuracy
+    output = torch.empty((m, n), dtype=dtype, device=device)
+    
+    for row in range(m):
+        # Extract nibbles for this row
+        row_data = qweight[row].to(torch.int32)
+        low_nibbles = (row_data & 0xF).long()
+        high_nibbles = ((row_data >> 4) & 0xF).long()
+        
+        # Lookup NF4 values
+        low_vals = lut[low_nibbles]
+        high_vals = lut[high_nibbles]
+        
+        # Interleave values
+        row_output = torch.empty(n, dtype=torch.float32, device=device)
+        row_output[0::2] = low_vals[:n//2]
+        row_output[1::2] = high_vals[:n//2]
+        
+        # Apply scales block by block
+        for block_idx in range(blocks_per_row):
+            col_start = block_idx * 64
+            col_end = min(col_start + 64, n)
+            
+            absmax_val = absmax[row, block_idx]
+            absmax32_val = absmax32[row, block_idx // 4]
+            scale = absmax_val * 0.00787401574803149606 * absmax32_val
+            
+            row_output[col_start:col_end] *= scale
+        
+        output[row] = row_output.to(dtype)
+    
+    return output